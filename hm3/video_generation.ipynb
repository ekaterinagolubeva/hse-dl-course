{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание № 3\n",
        "\n",
        "Создание / стилизация видео\n",
        "\n",
        "Ваша задача — создать нейросетевое видео с использованием одной из двух технологий, показанных в лекции:\n",
        "— Покадровая обработка существующего видео с применением ControlNet (предпочтительно) или Image-to-Image\n",
        "— Генерация видео с помощью латентной интерполяции из двух и более (желательно) текстовых запросов.\n",
        "\n",
        "Продолжительность видео — не менее 5 секунд (15 кадров в секунду).\n",
        "\n",
        "\n",
        "Критерии одобрения:\n",
        "— Оригинальность идеи\n",
        "— Качество исполнения\n",
        "— Сложность исполнения (использование ControlNet/нескольких промптов)\n"
      ],
      "metadata": {
        "id": "aZhoUS8AOE72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Скачивание данных"
      ],
      "metadata": {
        "id": "atG-MZXQPb6D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3qTo54j_NEOy"
      },
      "outputs": [],
      "source": [
        "# Пакет для установки датасетов из Huggingface\n",
        "!pip install -q datasets \"huggingface_hub[cli]\" torchcodec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "ds = load_dataset(\"UniDataPro/deepfake-videos-dataset\", split='train')"
      ],
      "metadata": {
        "id": "XXL3PMrKPhMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b8c587-a851-4eec-e707-660af40d1457"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(ds[0]['video'])"
      ],
      "metadata": {
        "id": "dyjZLm90SDBf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "1e6c593a-0897-40b2-faa6-5797ac448cf2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchcodec.decoders._video_decoder.VideoDecoder"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torchcodec.decoders._video_decoder.VideoDecoder</b><br/>def __init__(source: Union[str, Path, io.RawIOBase, io.BufferedReader, bytes, Tensor], *, stream_index: Optional[int]=None, dimension_order: Literal[&#x27;NCHW&#x27;, &#x27;NHWC&#x27;]=&#x27;NCHW&#x27;, num_ffmpeg_threads: int=1, device: Optional[Union[str, torch_device]]=&#x27;cpu&#x27;, seek_mode: Literal[&#x27;exact&#x27;, &#x27;approximate&#x27;]=&#x27;exact&#x27;)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/torchcodec/decoders/_video_decoder.py</a>A single-stream video decoder.\n",
              "\n",
              "Args:\n",
              "    source (str, ``Pathlib.path``, bytes, ``torch.Tensor`` or file-like object): The source of the video:\n",
              "\n",
              "        - If ``str``: a local path or a URL to a video file.\n",
              "        - If ``Pathlib.path``: a path to a local video file.\n",
              "        - If ``bytes`` object or ``torch.Tensor``: the raw encoded video data.\n",
              "        - If file-like object: we read video data from the object on demand. The object must\n",
              "          expose the methods `read(self, size: int) -&gt; bytes` and\n",
              "          `seek(self, offset: int, whence: int) -&gt; int`. Read more in:\n",
              "          :ref:`sphx_glr_generated_examples_decoding_file_like.py`.\n",
              "    stream_index (int, optional): Specifies which stream in the video to decode frames from.\n",
              "        Note that this index is absolute across all media types. If left unspecified, then\n",
              "        the :term:`best stream` is used.\n",
              "    dimension_order(str, optional): The dimension order of the decoded frames.\n",
              "        This can be either &quot;NCHW&quot; (default) or &quot;NHWC&quot;, where N is the batch\n",
              "        size, C is the number of channels, H is the height, and W is the\n",
              "        width of the frames.\n",
              "\n",
              "        .. note::\n",
              "\n",
              "            Frames are natively decoded in NHWC format by the underlying\n",
              "            FFmpeg implementation. Converting those into NCHW format is a\n",
              "            cheap no-copy operation that allows these frames to be\n",
              "            transformed using the `torchvision transforms\n",
              "            &lt;https://pytorch.org/vision/stable/transforms.html&gt;`_.\n",
              "    num_ffmpeg_threads (int, optional): The number of threads to use for decoding.\n",
              "        Use 1 for single-threaded decoding which may be best if you are running multiple\n",
              "        instances of ``VideoDecoder`` in parallel. Use a higher number for multi-threaded\n",
              "        decoding which is best if you are running a single instance of ``VideoDecoder``.\n",
              "        Passing 0 lets FFmpeg decide on the number of threads.\n",
              "        Default: 1.\n",
              "    device (str or torch.device, optional): The device to use for decoding. Default: &quot;cpu&quot;.\n",
              "    seek_mode (str, optional): Determines if frame access will be &quot;exact&quot; or\n",
              "        &quot;approximate&quot;. Exact guarantees that requesting frame i will always\n",
              "        return frame i, but doing so requires an initial :term:`scan` of the\n",
              "        file. Approximate is faster as it avoids scanning the file, but less\n",
              "        accurate as it uses the file&#x27;s metadata to calculate where i\n",
              "        probably is. Default: &quot;exact&quot;.\n",
              "        Read more about this parameter in:\n",
              "        :ref:`sphx_glr_generated_examples_decoding_approximate_mode.py`\n",
              "\n",
              "Attributes:\n",
              "    metadata (VideoStreamMetadata): Metadata of the video stream.\n",
              "    stream_index (int): The stream index that this decoder is retrieving frames from. If a\n",
              "        stream index was provided at initialization, this is the same value. If it was left\n",
              "        unspecified, this is the :term:`best stream`.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 23);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Проебразования видео"
      ],
      "metadata": {
        "id": "e72G1LlvUtCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.49 diffusers==0.32.2 accelerate opencv-python pillow mediapy\n",
        "!pip install -q controlnet_aux xformers safetensors tqdm\n",
        "!apt-get -qq install ffmpeg"
      ],
      "metadata": {
        "id": "p_gYtr4EUv7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d9085e-c366-4de6-f805-1f2f8556c501"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import mediapy as media\n",
        "from IPython.display import display, Video\n",
        "import gc"
      ],
      "metadata": {
        "id": "zbso7VuBWXqu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames_from_decoder(video_decoder, max_frames=None, target_fps=15, resize_width=512):\n",
        "    \"\"\"\n",
        "    Извлекает кадры из torchcodec VideoDecoder.\n",
        "\n",
        "    Args:\n",
        "        video_decoder: VideoDecoder объект\n",
        "        max_frames: максимальное количество кадров (None = все)\n",
        "        target_fps: целевой FPS для сэмплирования\n",
        "        resize_width: ширина для изменения размера кадров\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Получаем метаданные видео\n",
        "        # metadata это VideoStreamMetadata объект, не словарь\n",
        "        metadata = video_decoder.metadata if hasattr(video_decoder, 'metadata') else None\n",
        "\n",
        "        # Получаем общее количество кадров и FPS\n",
        "        if metadata is not None:\n",
        "            # Обращаемся к атрибутам, а не как к словарю\n",
        "            total_frames = metadata.num_frames if hasattr(metadata, 'num_frames') else 300\n",
        "            original_fps = metadata.average_fps if hasattr(metadata, 'average_fps') else 30.0\n",
        "        else:\n",
        "            total_frames = 300\n",
        "            original_fps = 30.0\n",
        "\n",
        "        print(f\"Исходное видео: {original_fps:.2f} FPS, {total_frames} кадров\")\n",
        "\n",
        "        # Вычисляем шаг для достижения целевого FPS\n",
        "        frame_step = max(1, int(original_fps / target_fps))\n",
        "\n",
        "        # Ограничиваем количество кадров\n",
        "        if max_frames is None:\n",
        "            num_frames_to_extract = total_frames // frame_step\n",
        "        else:\n",
        "            num_frames_to_extract = min(max_frames, total_frames // frame_step)\n",
        "\n",
        "        frames = []\n",
        "\n",
        "        print(f\"Будет извлечено {num_frames_to_extract} кадров с шагом {frame_step}\")\n",
        "\n",
        "        for i in tqdm(range(num_frames_to_extract), desc=\"Извлечение кадров\"):\n",
        "            frame_idx = i * frame_step\n",
        "\n",
        "            # Проверяем, не выходим ли за границы\n",
        "            if frame_idx >= total_frames:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Получаем кадр из VideoDecoder\n",
        "                # Обычно torchcodec использует get_frame_at() или get_frames_at()\n",
        "                if hasattr(video_decoder, 'get_frame_at'):\n",
        "                    result = video_decoder.get_frame_at(frame_idx)\n",
        "                elif hasattr(video_decoder, 'get_frames_at'):\n",
        "                    result = video_decoder.get_frames_at([frame_idx])\n",
        "                    # get_frames_at возвращает список/батч\n",
        "                    if isinstance(result, (list, tuple)):\n",
        "                        result = result[0]\n",
        "                else:\n",
        "                    raise AttributeError(\"VideoDecoder не имеет методов get_frame_at или get_frames_at\")\n",
        "\n",
        "                # Извлекаем тензор из Frame объекта\n",
        "                # В torchcodec Frame имеет атрибут data с тензором\n",
        "                # HACK: На разных данных по-разному работает :/\n",
        "                if hasattr(result, 'data'):\n",
        "                    # Frame объект с data атрибутом\n",
        "                    frame_tensor = result.data\n",
        "                elif hasattr(result, 'to_tensor'):\n",
        "                    # Альтернативный метод\n",
        "                    frame_tensor = result.to_tensor()\n",
        "                elif isinstance(result, torch.Tensor):\n",
        "                    # Уже тензор\n",
        "                    frame_tensor = result\n",
        "                elif isinstance(result, tuple):\n",
        "                    # Кортеж (frame, metadata)\n",
        "                    frame_obj = result[0]\n",
        "                    if hasattr(frame_obj, 'data'):\n",
        "                        frame_tensor = frame_obj.data\n",
        "                    else:\n",
        "                        frame_tensor = frame_obj\n",
        "                else:\n",
        "                    # Пробуем напрямую\n",
        "                    frame_tensor = result\n",
        "\n",
        "                # Конвертируем tensor в numpy array\n",
        "                if isinstance(frame_tensor, torch.Tensor):\n",
        "                    frame_np = frame_tensor.cpu().numpy()\n",
        "\n",
        "                    # Проверяем формат: обычно (C, H, W)\n",
        "                    if len(frame_np.shape) == 3 and frame_np.shape[0] in [1, 3, 4]:\n",
        "                        # Переставляем (C, H, W) -> (H, W, C)\n",
        "                        frame_np = np.transpose(frame_np, (1, 2, 0))\n",
        "\n",
        "                    # Нормализуем к диапазону [0, 255]\n",
        "                    if frame_np.dtype == np.float32 or frame_np.dtype == np.float64:\n",
        "                        if frame_np.max() <= 1.0:\n",
        "                            frame_np = (frame_np * 255).astype(np.uint8)\n",
        "                        else:\n",
        "                            frame_np = frame_np.astype(np.uint8)\n",
        "                    elif frame_np.dtype == np.uint8:\n",
        "                        # Уже в правильном формате\n",
        "                        pass\n",
        "                    else:\n",
        "                        # Другие типы - конвертируем\n",
        "                        frame_np = frame_np.astype(np.uint8)\n",
        "\n",
        "                    # Если 4 канала (RGBA), берем только RGB\n",
        "                    if len(frame_np.shape) == 3 and frame_np.shape[-1] == 4:\n",
        "                        frame_np = frame_np[:, :, :3]\n",
        "                else:\n",
        "                    frame_np = np.array(frame_tensor).astype(np.uint8)\n",
        "\n",
        "                # Изменяем размер, сохраняя пропорции\n",
        "                h, w = frame_np.shape[:2]\n",
        "                new_h = int(h * (resize_width / w))\n",
        "                # Делаем размеры кратными 8 (требование для Stable Diffusion)\n",
        "                new_h = (new_h // 8) * 8\n",
        "                new_w = (resize_width // 8) * 8\n",
        "\n",
        "                frame_resized = cv2.resize(frame_np, (new_w, new_h))\n",
        "\n",
        "                # Конвертируем в PIL Image\n",
        "                frames.append(Image.fromarray(frame_resized))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nОшибка при извлечении кадра {frame_idx}: {e}\")\n",
        "                # Пробуем следующий кадр\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nУспешно извлечено {len(frames)} кадров\")\n",
        "\n",
        "        if len(frames) == 0:\n",
        "            raise ValueError(\"Не удалось извлечь ни одного кадра!\")\n",
        "\n",
        "        return frames, target_fps\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Ошибка при работе с VideoDecoder: {e}\")\n",
        "        print(\"\\nИнформация об объекте VideoDecoder:\")\n",
        "        print(f\"  Тип: {type(video_decoder)}\")\n",
        "        print(f\"  Доступные методы: {[m for m in dir(video_decoder) if not m.startswith('_')][:10]}\")\n",
        "        if hasattr(video_decoder, 'metadata'):\n",
        "            print(f\"  Метаданные: {video_decoder.metadata}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "frames, _ = extract_frames_from_decoder(ds[0]['video'])\n",
        "assert len(frames) > 0, \"Не удалось извлечь кадры из видео\""
      ],
      "metadata": {
        "id": "fJqb3wl_T85W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "9c4bc2a828a644a985a1b872324ab422",
            "5da7ba6ee7474545b9e277df6ae8a754",
            "d11143240f254df1b88698697c306e6e",
            "7ba3cd6804214a8b96b4194b5f301f68",
            "0a43903e13f145ba80cfdd14d0a3ba13",
            "6f8c08ca938648a0bbdd21fb67f23f1a",
            "8cdfbbe08dba463fb4b90f811364eb2c",
            "d6e3951f78a04b628f04d19c66d8112e",
            "e91c5c66cb8d4d5ea4a291874048aa35",
            "fea1ef5306d946158c593b4142196608",
            "8c36223825674b1da0bb3cf4ddd5ad7f"
          ]
        },
        "outputId": "d5d29bd3-a887-485e-f2e8-f1d5b21364ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходное видео: 20.00 FPS, 200 кадров\n",
            "Будет извлечено 200 кадров с шагом 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Извлечение кадров:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c4bc2a828a644a985a1b872324ab422"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Успешно извлечено 200 кадров\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "81IgRdGFU5_Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обработка видео с помощью моделей Stable Diffusion"
      ],
      "metadata": {
        "id": "3n10fD5oXEEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import (\n",
        "    StableDiffusionControlNetPipeline,\n",
        "    ControlNetModel,\n",
        "    UniPCMultistepScheduler,\n",
        "    StableDiffusionPipeline,\n",
        "    DDIMScheduler\n",
        ")\n",
        "from controlnet_aux import CannyDetector, OpenposeDetector, HEDdetector\n",
        "\n",
        "\n",
        "# Загрузка процессора для ControlNet:\n",
        "processor = CannyDetector()\n",
        "controlnet_model_id = \"lllyasviel/sd-controlnet-canny\"\n",
        "\n",
        "print(f\"Загружаю ControlNet модель: {controlnet_model_id}\")\n",
        "\n",
        "# Загрузка ControlNet\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    controlnet_model_id,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "print(\"Загружаю Stable Diffusion pipeline...\")\n",
        "\n",
        "# Загрузка основной модели Stable Diffusion\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None,\n",
        "    use_safetensors=True\n",
        ")\n",
        "\n",
        "# Оптимизация для GPU\n",
        "print(\"Настройка оптимизаций...\")\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "# Перемещаем на GPU\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# Включаем оптимизации памяти\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "print(\"✅ Модели загружены и готовы к работе!\")\n"
      ],
      "metadata": {
        "id": "iDx6tT4uXEnN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955,
          "referenced_widgets": [
            "4c9d69e0b8ca425eb2de64fbccc1110b",
            "f6ad935e509f4702be89cd1469a924c6",
            "02bc7b09c90241b1a697389c3e8d2b05",
            "7f57c486576c4c3cb9cd5831f8ae5427",
            "79bd1a9ee9974d128572f018256ae393",
            "2024ec0b789547afa61fe58ccf58f748",
            "76dcf4b75c1840a98da485bafff9149c",
            "2662fc8f49f94a029fd7489fb15108a0",
            "2305e137bb8741dc8fa9a486bf8003a8",
            "2f805f51565a4032bba769b5fd7a87cf",
            "642eb84ee8384f69b17ac853095c1462",
            "878f44e51a7f4cb0889e8e59052e01b9",
            "6af588b7dab04a81a5dd3fe223ad48b5",
            "c24a1ea4b3ed41778859e076f39b9914",
            "39f33be9e6524bb7a4409b98e304e9d0",
            "caf6b447e9d54f5d8c38e2f9fbe936c2",
            "11cfb3a47c244764a5c722baffef03d2",
            "b282c28e777c4dd099f913090411646e",
            "3a54e1b488ea4fd5b12874b6d286e4d3",
            "f8cfe65032dd4acdae2666efa48909ae",
            "032376302e664ff08df742e005a5cf96",
            "4516a49028ea447789988e7bf3dd4227",
            "be97530dc072452db6003bd46541d7cf",
            "2a58b4bdfd5b4dba92afdb3a6a398a54",
            "c892b0178d654450a4d7b20187398826",
            "3a07624e1b1f4481b49833935a5f9132",
            "1cc10c46774044368ffcd641a70c2260",
            "e357ce8ce87d485a88a04690658f62af",
            "4c9853744a284570b8aa7bf5018500af",
            "1bf33e3febc14e38af407a5d39766e2b",
            "5b777643659b4f80984dea38cb5411f7",
            "c8ba8572fad24a0ab3ab087dc4d39cfa",
            "0bcd728953db4029a8ecb9a90c16a29d",
            "f20316294b2b414f858994be4f0ffb36",
            "d11dc6c0b05d47ffb05be5aef7e623e5",
            "6758cb2368c94730a67de65279e3f3c1",
            "87698e6cc4fa4813b242a18a39829edd",
            "915661261fbb43f1a06540e9f4b4e235",
            "e0c92507826e4c39914210c1eae8e605",
            "747227ebccdf4c1885ca108f6ddf7846",
            "e0c335553cd44896b4848d70b3333ad3",
            "d55d9caa08184ae28ee7d0315822d6e1",
            "cdb6baddf7d942ea905ae6e549a59cd2",
            "5805f5913c2b48cfbac58e4f74a2c1b2",
            "187800dc061440a78d7b6bf69df124cd",
            "9a57cd454c904cde8505148698d387dd",
            "05fc5d7f186149f3928e56d24c1aac87",
            "b43448ee7c214e7580375fb1f8673044",
            "085f66cb33e44813aedcf6fc49e47c65",
            "a97e251467c24b1382527476e72b9d71",
            "6eafd2305e7f48139ac1e05b5847b337",
            "56f6dbd3c0c848f383db2590747604ef",
            "19a51309da2046508c6df0f788f94d31",
            "e2abd5f6e3954ef9bc6c61c97905b3d4",
            "8d9448c7ad624b96bcbe0d640b9e458f",
            "2a706afc3d334886ad065b505b60417e",
            "4cee2f228e4e4f4f93d11f9bb6ff0a7d",
            "6215173273ae4c718e62abd45efdfb99",
            "2ef67e6629ea4f4a87a06d693bbe0e26",
            "0a177b90d3f743568d3f2add044879ca",
            "8f4a8249a6b3469e84eb741b59e02386",
            "e8c0d13f2ec7439fb526b217f77be763",
            "e81b36bba4c245098755c80c1ae7a2d4",
            "9523c20864cf41e08934abf6d2a74da0",
            "06f03bde3eb84826b6303aeb4243edce",
            "44c2a56cbfd1428080bd958fab321773",
            "45482a8074df4dbbb7f41dead6fd6b4f",
            "53decc9d355a44078efec3ea8c3466e4",
            "4c999b9b7aa94eeabfd89412f0e11162",
            "45b7f9c45f69460682cc0b499eb5e92e",
            "968a54ad2cd54241aaf4c4a39a0ae104",
            "41f99124113141208cb77e20e8c8968b",
            "b95f619a1e31473292605f2229eb2a80",
            "16181bf0043e42a0917b8eea9d6328be",
            "16e4a8c332974fccae2953a23bbc2028",
            "3e823f616f024fd48b5309e84d28399e",
            "048a29dc84de4193b4025850dfe2f77e",
            "cbcac7d3eb57420384349e7aee702608",
            "6020ffe35bf240749bb95e75575942da",
            "8f3d6be2a7c2440d8cc12f2ecd3f5f99",
            "fe2621ed9ffb4f64aac020a6bb7261bd",
            "5e2f6c04b8b146e79a0c7f6d64912bf3",
            "34fb5365830040fd8ceb7ed0d1932fce",
            "de78b816f37e49df98e8cb064c6f5356",
            "fed1961c1fe54c4a8f05fcf52b08e869",
            "cf1f28844da14b5c8844fa77b3bdfe96",
            "3c51c27fa9254961b81090bb2450f287",
            "99309e983d904078806f9cb6aaa400ef",
            "55bdd9c9147d43568c8750b09c60f25e",
            "809a285809364802b120c5011af9e9b6",
            "e22d89077cfd4ecfb5c24c0cc2d71040",
            "a455120a6e984eff9b6435ed570b1ba7",
            "32aa1430cd4b462bab6c425fe7e03bcd",
            "048ada793717484ba8714cd33d1ac2d5",
            "f3f68b483e5c4c7d9f004c380850901e",
            "2bb80008ea984cf28b27c1385f1c20fe",
            "77575cf6788d4482ab487f06218c8a1e",
            "6c54093cb05f4cb1998ee9ed0b67b75b",
            "9f83a28c55244fef9a9611c3ed9bdbde",
            "09a057bcc40c4699b46b15a629a1b778",
            "d124f39e43f5403cbe01b5925f1b6cef",
            "18463b15d3534c42897b35edcb08b10c",
            "ba9a5d9630964928a52e2c8e8362be06",
            "7d9c16971d3d42b795f9cffce3a47e81",
            "e6ea0064df8040f0b089817266866447",
            "364ceff3a8b34c929d79bfc9ae9bdcf0",
            "236c167cef1b4bd3b2ab65a92273cfe5",
            "07117ec3f834408a851cb5a613a22fdc",
            "d330ac1de7e1410ba106c8caf78010f1",
            "9e24d76f61f74bbeb58ae633d3e7ec81",
            "330629cd38844bffbafd59a8987962cd",
            "6221eae3beb54c3bb3d87d21605d4a4e",
            "9bf146cc2f03406cb8c60dd652b4af60",
            "64989474935c44ec9a87cb460d6b118a",
            "b6184c81ebd64c47b6ba0d9febdf3590",
            "35ccfa441eec4372afa6caa2f9b4d5fd",
            "79674ee3b0d84e49bb858d2a92b97851",
            "a487b78dbd3e49d3950abf31b3c2531c",
            "c365adae1a0a4416ac7e23f46c79559d",
            "5f90a678c28b4ebb9d8a82dc04d0c8d3",
            "9f7df63744c84a6a9fc600f9318d40b4",
            "23239520d30549f495b2426cad6bcb03",
            "cb0f1b80edb54192b7535a733ce5afc2",
            "e47b8961729a464f9e076646c0b65340",
            "8c5006f362794b11be6c599a8b77b951",
            "9aeb7642817242fcb9dc46386eefaa5d",
            "1e8631097dcc49ebb285469944513889",
            "c864ffbc0c314eb88c28be54bf9f6a63",
            "f397b55746a340fe8807ba997a072ba7",
            "e50f15871f3641de9be673f7900aa41b",
            "caf310c9a7554ec39c2d1fc4ee30b501",
            "35ace8a3a8464db29f532ebed3a3d6a9",
            "8b021efa6f0948f199858c9c2cfdafa0",
            "22d3c4bc632c4abaa2c4e831b74c4d22",
            "361518bafc1643a186f61b4460ca397d",
            "1fd2a494ad164bada55c524c63a27a83",
            "3943f27148c848299e766afa838f3dbc",
            "50490620158245dc86f20e15887e5d4d",
            "9b9695aa76e94d77a986846a62c9e526",
            "dea1e12afbc14537a97615f55322447c",
            "c3d5d27a55e94219ad1894b0f2e4e229",
            "5163272882df47a7be860bfa9e615fb1",
            "472bec7e8cf04620abc3010e6620d5e0",
            "63fbe7c621df47339ed7aee7c02cb141",
            "e2313600a98940e3b98f317b75f40472",
            "d1750aa8eb8443b8b7ce0b3d95a70689",
            "4c06a4281dcf4c19beb0b791a6c08bb1",
            "6b11222f5a1d4a2c9eae5587846bc14d",
            "f6afe547407d44cbb70707a0ffe87d2d",
            "b84762f6d9f144178b8f2d79e42280bc",
            "386c17eb9e4c45ee920377aa3007c2b1",
            "5c66be70189c4d94ba8fe4b70a451221",
            "526fbd714b0f47f1bb13ffdb526691b3",
            "b03de4631dbc43fcbd417962626e4bec",
            "5b7aff7b6671499792773ad748ba40c6",
            "39e43ff80e0442d6a585e26dec591c32",
            "a82fec34ffa54ceabcfcdd5a231c14fb",
            "23fad72d1318410785b72c3eea9e85c8",
            "b8b66820f16e42efaa5cbc77ae01df7c",
            "1d46757732e24903a1a00f4972def9aa",
            "639dd3b4aeb14560b2f737adae92f084",
            "eb3ebcc2cdde433e9490b18a4d7a6949",
            "64a71fc9eefe4ab2ad35c0bed8d545c4",
            "ba86e592edcf424aba5967aa3e561f17",
            "f811bae73f7d49209c4e0dfae8996681",
            "f2fd245f7e3d4b9cbfd43b62f83b4f99",
            "894382dbf6204f6fb5b47ec2e7eeaab0",
            "e2d3e80748504459b804cb229975cd68",
            "a2237afee25c42709359ac40fe3ab477",
            "f3e884835d954bf68665e07d69178db0",
            "a5c7aac11eb64f18ab382d72f949fc20",
            "4b2ed0bf3279472397e105e3f3374a82",
            "dd0b834fe00144bd950ad614e7a30ea7",
            "cc933cebed9a4805b31d69978c8f9f4d",
            "9fefcdafc94a4256bdab60ff373ee8cf",
            "ef503a0aa53c40da838617d07806e650",
            "ad40d2db38db482ea0aa84b5a53146ab",
            "6e74d3a0ff1841e7869055d506a5ba5b",
            "01799ce75b0c471e8a75908991863868",
            "8b23f83d81734ba8b457da02a2c2ec93",
            "88769ca74ed848578447d5971a02f47d",
            "2b5ec74d92b14526a94a0de11d9ed739",
            "b4d124e04dde4ba79cfdef7d4896f581",
            "6fb0fd8b20384de280ace4c44cbd95cd",
            "a257dfd41d1e4c48b2b825c3218afee7",
            "ae9b2e6ce43d4023b97f7d026e0d7398",
            "9853176be5414e97ad25e40972ded983"
          ]
        },
        "outputId": "0225e426-502c-41c2-dbee-354e5d589405"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.12/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.12/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.12/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.12/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружаю ControlNet модель: lllyasviel/sd-controlnet-canny\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/920 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c9d69e0b8ca425eb2de64fbccc1110b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "diffusion_pytorch_model.safetensors:   0%|          | 0.00/1.45G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "878f44e51a7f4cb0889e8e59052e01b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружаю Stable Diffusion pipeline...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be97530dc072452db6003bd46541d7cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f20316294b2b414f858994be4f0ffb36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "187800dc061440a78d7b6bf69df124cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a706afc3d334886ad065b505b60417e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45482a8074df4dbbb7f41dead6fd6b4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbcac7d3eb57420384349e7aee702608"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55bdd9c9147d43568c8750b09c60f25e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "text_encoder/model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09a057bcc40c4699b46b15a629a1b778"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "330629cd38844bffbafd59a8987962cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23239520d30549f495b2426cad6bcb03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b021efa6f0948f199858c9c2cfdafa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63fbe7c621df47339ed7aee7c02cb141"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unet/diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b7aff7b6671499792773ad748ba40c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2fd245f7e3d4b9cbfd43b62f83b4f99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad40d2db38db482ea0aa84b5a53146ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Настройка оптимизаций...\n",
            "✅ xformers включен\n",
            "✅ Модели загружены и готовы к работе!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "\n",
        "\n",
        "class Prompt(TypedDict):\n",
        "    content: str  # Содержание промпта\n",
        "    suffix: str  # Суффикс, который будет ставиться в сохраненном файле\n",
        "\n",
        "\n",
        "positive_prompts = [\n",
        "    Prompt(\n",
        "        content=\"symmetrical composition, pastel color palette, vintage aesthetic, perfectly centered framing, whimsical atmosphere, retro production design, meticulous art direction, nostalgic feel, quirky characters, detailed background, soft lighting, cinematic, 35mm film\",\n",
        "        suffix=\"atmosphere\"\n",
        "    ),\n",
        "    Prompt(\n",
        "        content=\"black and white, high contrast, dramatic shadows, chiaroscuro lighting, 1940s aesthetic, film noir style, mysterious atmosphere, venetian blinds shadows, hard light, cinematic composition, detective movie, moody, dramatic, vintage crime thriller\",\n",
        "        suffix=\"noir\"\n",
        "    ),\n",
        "    Prompt(\n",
        "        content=\"1950s futuristic vision, atomic age aesthetic, streamlined design, chrome and pastel colors, ray guns and rockets, mid-century modern, space age design, optimistic future, vintage sci-fi illustration, retro technology, sleek lines, art deco influence\",\n",
        "        suffix=\"retrofuturism\"\n",
        "    )\n",
        "]\n",
        "\n",
        "negative_prompts = [\n",
        "    Prompt(\n",
        "        content=\"out of focus, motion blur, defocused, soft focus, bokeh mistake, unfocused subject, camera shake, blurry details, lack of sharpness, gaussian blur\",\n",
        "        suffix=\"blured\"\n",
        "    ),\n",
        "    Prompt(\n",
        "        content=\"warped, deformed, distorted proportions, stretched, compressed, unnatural perspective, morphed, twisted geometry, wrong anatomy, malformed structures, broken composition\",\n",
        "        suffix=\"distorted\"\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "# Параметры генерации\n",
        "num_inference_steps = 8\n",
        "guidance_scale = 7.5\n",
        "controlnet_conditioning_scale = 1.0\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "8C8YiCTQX2Zu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_frames_with_controlnet(frames, processor, pipe, prompt, negative_prompt,\n",
        "                                   num_inference_steps=20, guidance_scale=7.5,\n",
        "                                   controlnet_conditioning_scale=1.0, seed=42,\n",
        "                                   use_fixed_seed=True):\n",
        "    \"\"\"\n",
        "    Обрабатывает кадры с помощью ControlNet.\n",
        "\n",
        "    Args:\n",
        "        use_fixed_seed: если True, использует один seed для всех кадров (больше согласованности)\n",
        "    \"\"\"\n",
        "    processed_frames = []\n",
        "\n",
        "    for i, frame in enumerate(tqdm(frames, desc=\"Обработка кадров\")):\n",
        "        # Применяем процессор ControlNet\n",
        "        control_image = processor(frame)\n",
        "\n",
        "        # Стратегия 1: Фиксированный seed для всех кадров\n",
        "        # HACK: Это нужно чтобы сделать переходы между фреймами более плавными по стилю\n",
        "        if use_fixed_seed:\n",
        "            generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "        else:\n",
        "            # Стратегия 2: Близкие seed для соседних кадров\n",
        "            frame_seed = seed + i\n",
        "            generator = torch.Generator(device=\"cuda\").manual_seed(frame_seed)\n",
        "\n",
        "        # Генерируем кадр\n",
        "        output = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            image=control_image,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
        "            generator=generator\n",
        "        )\n",
        "\n",
        "        processed_frames.append(output.images[0])\n",
        "\n",
        "        # Периодически показываем прогресс\n",
        "        if i % 10 == 0 and i > 0:\n",
        "            print(f\"Обработано {i}/{len(frames)} кадров\")\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return processed_frames\n",
        "\n",
        "\n",
        "def generate_video(video, positive_prompt, negative_prompt):\n",
        "    \"\"\"Генерирует видео\"\"\"\n",
        "    frames, fps = extract_frames_from_decoder(video, max_frames=120)\n",
        "\n",
        "    processed_frames = process_frames_with_controlnet(\n",
        "        frames, processor, pipe, positive_prompt, negative_prompt,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    return processed_frames"
      ],
      "metadata": {
        "id": "1iFXR-mzX3-J"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from uuid import uuid4\n",
        "\n",
        "\n",
        "def create_video_from_frames(frames, output_path, fps=15):\n",
        "    \"\"\"\n",
        "    Создает видео из кадров.\n",
        "    \"\"\"\n",
        "    # Конвертируем PIL изображения в numpy массивы\n",
        "    frames_np = [np.array(frame) for frame in frames]\n",
        "\n",
        "    # Создаем видео с помощью mediapy\n",
        "    media.write_video(output_path, frames_np, fps=fps)\n",
        "\n",
        "    print(f\"✅ Видео сохранено: {output_path}\")\n",
        "    print(f\"   Длительность: {len(frames)/fps:.2f} секунд\")\n",
        "    print(f\"   FPS: {fps}\")\n",
        "    print(f\"   Кадров: {len(frames)}\")\n",
        "\n",
        "\n",
        "def gen_pipe(video, positive_prompt: Prompt, negative_prompt: Prompt, fps: int = 15) -> str:\n",
        "    positive = positive_prompt.get('content')\n",
        "    negative = negative_prompt.get('content')\n",
        "\n",
        "\n",
        "    processed_frames = generate_video(video, positive, negative)\n",
        "\n",
        "    pos_suffix = positive_prompt.get('suffix')\n",
        "    neg_suffix = negative_prompt.get('suffix')\n",
        "    output_video_path = f\"{pos_suffix}_{neg_suffix}_{uuid4()}.mp4\"\n",
        "    create_video_from_frames(processed_frames, output_video_path, fps=fps)\n",
        "\n",
        "    return output_video_path"
      ],
      "metadata": {
        "id": "zTw5BUVYlw08"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_video_paths = []\n",
        "\n",
        "for video in tqdm(ds['video'][:3], desc='Обработка видео'):\n",
        "    for pos, neg in tqdm(zip(positive_prompts, negative_prompts)):\n",
        "        output_path = gen_pipe(video, pos, neg)\n",
        "        output_video_paths.append(output_video_paths)"
      ],
      "metadata": {
        "id": "ZsYSDEiAaRPE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593,
          "referenced_widgets": [
            "6740f36a8c5a4520b25ad1a2651fa551",
            "d161e39a847c4597b6681df15ac2cc6c",
            "01edc80d5e114944973c621145158e06",
            "057f5fcde179442e87b670f5a28d9691",
            "2b20b1384a4c4c079f28e486529ee85a",
            "854b32cadc8d45cb94be3b576c4fd8bb",
            "30e41de3c7ed4c8d9d672b2bcf160e52",
            "14dc6859f3474d6dba34fb3f4b08b10e",
            "6ee32e267b964662bf36be4e0b62fff4",
            "f0375abb895f4cbaacab1ece20cfb2c4",
            "6a71a451192544ee8a8f09dee039ddc7",
            "4ca38d9de1f84ea6861ea7151e673aad",
            "2cddc1d9093342e88f292f616110d385",
            "975a1bf7be1443c385ea069b5f56886a",
            "3c4835998a864f628cb2b14bd1efb900",
            "323adea0bdaf45ed81e775b59c1d8af4",
            "50444820e9bd4765810dab2f2cec3d42",
            "58df06244b56406797437c389e43ef89",
            "20a12024eb4c43f880c7e8cb266d3255",
            "da9dcd1acc9640988c6d17261ec5fc01",
            "9f876b1b1a194434b87c43ad27d741df",
            "993acc2213b24396be6d6bf76312c7ba",
            "5c2f7cf96b46411281a140b4530fce4d",
            "1afd488f6d7c4cb8b73fa8731b23ca72",
            "22890c76aff048b5be97b2d5dabf558b",
            "c2e5b0ef53f6406fbf8b1ccf7173e989",
            "4b31ffe1181844f8bcdf7af09dcfb721",
            "b2f2a80a85bf462088c13471b432c436",
            "8206243ebfab4e689cd20945411e58f3",
            "d746eadf071c43bf86be4f5ea1d08f91",
            "85395584be804391bcdb8075717d22f4",
            "0888c198efb2474590838470ec7a5563",
            "645844a5439541bf81c87a883a13bf34",
            "2fd48b42b56f49cbb3574573aabef437",
            "876676a59b7b4a0d86b1ddc54b5a8e3f",
            "cf89b8d7fa3940a998b3d9a3bfceabe6",
            "00d7966dfe034bad9a3d2fb5fe384469",
            "b59582870b924d679310c744e6eaf036",
            "be9f64e1beec441aa92f83901e012101",
            "8aeab97b4e00447fad6acc368195f1c7",
            "7ea25a0f41af4014900c79a1e76165c1",
            "8b5e5256caeb4016988d42f93f98b5f0",
            "b7327b2249e649f78864543cf0eb30c8",
            "371eff29e4ae44f8ae4383067c53dec9",
            "c08e9d0523d746bbb74cb370bd7bb0cc",
            "d723383b00e440fc83334bdf50f0bb67",
            "2a360afc4f204f51839833c263c6e0b0",
            "6b72d2f984fc4f53b65cc17403cf110f",
            "baad26810a3847cbb1fbca8a0fef3053",
            "9cee623e1a3142bcb23f44fd7d1ea60d",
            "8ffcb8392dba4b40929a04bf7e6ecaa8",
            "a76c8d62ebf34f4d9c79c846b04ec548",
            "b00753516bb34e5aae220b47057a98d2",
            "9e8b4499d8524732a1b868deddd75cb4",
            "4cd3add42b93448c9785e905377cc47b",
            "78508dcba8b94fed8fad9f9d3157120e",
            "1133cf48a69b495d90bea6d383f2ae43",
            "c7b77653f9cc4c2b990dd69c656ee1e8",
            "8c4587e6bf774ec5b823d3c4744e9e7b",
            "ea4a7986153c4ec6aad303d2270b8615",
            "2d6b3b709839454f96e9b73cde3c10e9",
            "9895b8f0efb24d76988a52408267d528",
            "24b5c5e4ba6d4370acb57f055f655bad",
            "16524281ecac44cc81cc3ae9b3b1c4a3",
            "98b1c39f224e413ba68e6b710ebc5ce8",
            "fcf1dbdff9034e65ba1f44bbb229e46f"
          ]
        },
        "outputId": "bddb2881-493a-4d79-b0a8-5b4484fd5140"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Обработка видео:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6740f36a8c5a4520b25ad1a2651fa551"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ca38d9de1f84ea6861ea7151e673aad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходное видео: 20.00 FPS, 200 кадров\n",
            "Будет извлечено 120 кадров с шагом 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Извлечение кадров:   0%|          | 0/120 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c2f7cf96b46411281a140b4530fce4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Успешно извлечено 120 кадров\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Обработка кадров:   0%|          | 0/120 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fd48b42b56f49cbb3574573aabef437"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c08e9d0523d746bbb74cb370bd7bb0cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78508dcba8b94fed8fad9f9d3157120e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2855708721.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Обработка видео'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0moutput_video_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_video_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4211845534.py\u001b[0m in \u001b[0;36mgen_pipe\u001b[0;34m(video, positive_prompt, negative_prompt, fps)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mprocessed_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpos_suffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositive_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'suffix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2332714995.py\u001b[0m in \u001b[0;36mgenerate_video\u001b[0;34m(video, positive_prompt, negative_prompt)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_frames_from_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     processed_frames = process_frames_with_controlnet(\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mnum_inference_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_inference_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2332714995.py\u001b[0m in \u001b[0;36mprocess_frames_with_controlnet\u001b[0;34m(frames, processor, pipe, prompt, negative_prompt, num_inference_steps, guidance_scale, controlnet_conditioning_scale, seed, use_fixed_seed)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Генерируем кадр\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         output = pipe(\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/pipelines/controlnet/pipeline_controlnet.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, image, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, controlnet_conditioning_scale, guess_mode, control_guidance_start, control_guidance_end, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mcond_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrolnet_cond_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcontrolnet_keep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m                 down_block_res_samples, mid_block_res_sample = self.controlnet(\n\u001b[0m\u001b[1;32m   1280\u001b[0m                     \u001b[0mcontrol_model_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/controlnets/controlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, controlnet_cond, conditioning_scale, class_labels, timestep_cond, attention_mask, added_cond_kwargs, cross_attention_kwargs, guess_mode, return_dict)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdownsample_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownsample_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"has_cross_attention\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdownsample_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 sample, res_samples = downsample_block(\n\u001b[0m\u001b[1;32m    809\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0mtemb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/unets/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m                 hidden_states = attn(\n\u001b[0m\u001b[1;32m   1335\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/transformers/transformer_2d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 )\n\u001b[1;32m    441\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 hidden_states = block(\n\u001b[0m\u001b[1;32m    443\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0mnorm_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             attn_output = self.attn2(\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mnorm_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/attention_processor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mcross_attention_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattn_parameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         return self.processor(\n\u001b[0m\u001b[1;32m    589\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/models/attention_processor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3090\u001b[0m         \u001b[0;31m# linear proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3091\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3092\u001b[0m         \u001b[0;31m# dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Очень долго обрабатывается :/"
      ],
      "metadata": {
        "id": "Eic-Tk4LpMRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def create_mp4_archive(output_filename='videos.zip'):\n",
        "    \"\"\"\n",
        "    Собирает все MP4 файлы из текущей директории в ZIP-архив.\n",
        "\n",
        "    Args:\n",
        "        output_filename: имя выходного ZIP-файла\n",
        "\n",
        "    Returns:\n",
        "        Количество добавленных файлов\n",
        "    \"\"\"\n",
        "    mp4_files = glob.glob('*.mp4')\n",
        "\n",
        "    if not mp4_files:\n",
        "        print(\"MP4 файлы не найдены\")\n",
        "        return 0\n",
        "\n",
        "    with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for mp4_file in mp4_files:\n",
        "            print(f\"Добавляем: {mp4_file}\")\n",
        "            zipf.write(mp4_file, os.path.basename(mp4_file))\n",
        "\n",
        "    print(f\"Создан архив {output_filename} с {len(mp4_files)} файлами\")\n",
        "    files.download(output_filename)\n",
        "    return len(mp4_files)\n",
        "\n",
        "create_mp4_archive('controlnet_videos.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "q2DZYPWIio7b",
        "outputId": "f3b7d2f4-17fb-4d5f-dc88-a5e6767e75fc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Добавляем: noir_distorted_f2f44151-8862-409d-824c-eb91aa63b103.mp4\n",
            "Добавляем: atmosphere_blured_87f78884-75ae-44dd-8742-11bfa51a1189.mp4\n",
            "Добавляем: atmosphere_blured_f8841855-caf1-44d1-8565-bddb97e989a2.mp4\n",
            "Добавляем: atmosphere_blured_23d97556-039a-4eb8-b3c0-c4354b599fef.mp4\n",
            "Добавляем: noir_distorted_8938de6e-e5cb-4ebc-934d-46c8dd1fe1a4.mp4\n",
            "Добавляем: noir_distorted_c178b3bd-371d-4917-8901-9b956af13230.mp4\n",
            "Создан архив controlnet_videos.zip с 6 файлами\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_11d22294-4af2-46c2-bab2-8596d0824f03\", \"controlnet_videos.zip\", 8031236)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ASV3gjhptMCk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}